「推論にもとづく談話処理」グループの研究の焦点
  大和処理において、いかに大規模知識を使いこなすか

談話処理
書かれていないこと
- 信念
- 
- 代名詞の省略
…の復元

- 固有表現認識
- 談話関係解析
- 多義性解消
- 照応解析
- ??? # 既存の枠組みにはまらない
- ???

DARPA -- DEFT
  書いていないことを explicit にするのは国防上も重要
  需要がある

WWW --知識を抽出--> 知識データベース
      (因果関係、常識的知識)
      知識獲得研究の発展
WWWの超大規模化

* 世界知識を使う
従来の世界知識の使い方
「素性ベースアプローチ」
テーブル・ルックアップ

照応解析
  素性ベクトル
  hypernym-of:1
  causality-jump-fall:1
  → SVM等

問題点
  背景知識 には様々な粒度
  談話 様々な粒度の事実

「単純な知識の組合わせ」で

→
推論ベースアプローチ FOLベース
  仮説候補生成
  仮説選択

Interpretation as Abduction
文を解釈する = 文に対する最良の仮説を求める

* 共参照解析への応用
うぃのぐらっどすきーまちゃれんじ
照応解析 簡単な問題は研究が進んでいる
難しい部分に取り組んでいる
Levesque+
Turing test はもうダメだ これをやれ

事象間関係知識 X hunt -> X eat

** 

知識 2億ある
ClueWeb12の一部 2億文書7億文

動詞の周りに書いてある色々なこと が似ていることを
  知識を作るときにも使うときにも文脈を使う

→ 1,2年のうちにabduction に入れたい

* Yamamoto 仮説推論における談話処理…における基盤技術
- [ ] 学習!

談話減少をどのような論理表現に変換するか
知識をどのような公理によってあらわすか
必要となる大量の背景知識をどう集めるか
○ モデルパラメータをどう学習するか / 背景知識の重みをどう与えるか (機械学習)
○ 大規模推論を用いた推論により高速におこなうには

1. 仮説の候補を生成
2. 仮説の良さを評価

評価関数のパラメータの調整手法が未確立

重みつき仮説推論 Hobbs+,93

背景知識の重み
  逆向き推論を適用したときの推論の信憑性 小さい方がよくありそう
  - [ ] 世界(B)によらない??
観測の重み
  観測の仮説らしさ (それらの観測をどのくらい深く説明したいか)

- [ ] 二つの理由が矢印に導かれる場合は?
      理由は排反ではない

逆向き推論と単一かを繰り返して、合計コストが少ないほど良い仮説、とする
- [ ] 五十嵐研で高速化できる?

- [ ] 初期値?
- [ ] 訓練データ、人が作る?

- [ ] back propagation のときに正例を入れない理由は?

* どうやって高速に推論するか
- 組合わせ最適化問題

** ILP
リテラルの有無を変数
- [X] 一意? ==> ひとつの大きな木の1ノード

** + Cutting Plane Inference

** A*
- [ ] 最適解は必ず単一化が起きる? ==> 直感的にはその通り 証明まだ
