* ???
- 計算量

- 問題設定
  - 構造学習問題の難しさ→分類問題へのアプローチの次に挟む
  - HMM の利点/欠点
  - CRMs の利点/欠点
  - structured perceptron の利点/欠点

- HMM
  - Viterbiの式
  - 隣り合った品詞の関係性のみを記述
  - マルコフ性を仮定

- CRF 複数の素性を持たせた場合のグラフィカルモデルが分からない
- CRFの特徴
  - 識別モデル
  - 複数の素性を取り込める
  - ラベルバイアスを解消
- 学習
  - 対数線形モデル→正則化項
例えば「This is a pen」という文書の各単語に「This(代名詞) is(動詞) a(冠詞) pen(名詞)」のように品詞ラベルをつける問題

- 入力が構造を持つ場合
  - kashima 3-3
- (0,1,0,0,0)を回帰する、な話は…?
http://ibisforest.org/index.php?%E5%A4%9A%E3%82%AF%E3%83%A9%E3%82%B9
- 多クラスロジスティック回帰の別表現、理解できてない
  - kashima 3-1? の式で理解できない
- 多クラスロジスティク回帰/対数線形モデル/最大エントロピーモデル の関係性
- MEMM ?

- グラフィカルモデル
  確率変数をノードとし，確率変数間に統計的な依存関係を有向辺で表したグラフを考える．このグラフに基づいて，複数の確率変数の同時確率や条件付き確率を扱うためのモデル．
http://ibisforest.org/index.php?%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%82%AB%E3%83%AB%E3%83%A2%E3%83%87%E3%83%AB


- オンライン学習とバッチ学習
  - 岡野原

- Structured Perceptronの理論的評価 (徳居さんレジュメ)

- Beamer

- 入力が構造を持っていた場合?
- グラフ ?
- 順位付構造 ?

- Logistic回帰

- x/yの長さ、毎回違うよね…?

- この手法の cf.
  - [ ] 最大エントロピーモデル
  - [ ] CRFs

- 品詞タグ付け
  - JJ 形容詞
  - NN 名詞系
  - RB 副詞系
  - VB 動詞系

  - DT 前置詞

* 目次
- 隠れマルコフモデル(HMM): 系列ラベリング問題の生成モデルによる形式化
  - 生成モデル
  - 欠点: モデルに複数の素性を付与できない

  - Viterbiアルゴリズム

- 条件付確率場(CRF): 系列ラベリング問題の識別モデルによる形式化
  - 利点: 多くの素性が利用可能
  - 欠点: 学習が遅い

  - 最大エントロピーモデル, 対数線形モデル = 多クラスのロジスティック回帰も出る?

- 隠れマルコフパーセプトロン(structured perceptron): 学習の速いCRF
  - 最適化問題を解かずにパラメータを学習する

  - 利点: 多くの素性が利用可能
  - 利点: 学習が速い

* タイトル、、、
Structured Perceptron
[Collins, 02]: 系列ラベリング問題へのパーセプロトンの適用

* 構造学習(structured output learning): 出力が構造を持つ関数を学習する
- 教師付き学習(supervised learning)

  - 識別モデル(discrimiative model)
    - 識別関数の学習
      - 学習
        - {(x_i,y_i)}_i を見て、f:X→Y(のパラメータ)を学習する
      - 予測
        - y = f(x)
	# クラスの分類問題なら、ここでも argmax_y では…

    - 条件付確率場の学習
      - 学習
        - {(x_i,y_i)}_i を見て、P(Y|X)(のパラメータ)を学習する
        - 条件付確率を直接推定する
      - 予測
        - y = argmax_y P(y|x)

    - fの値域は連続値(回帰)または離散値(分類)

  - 生成モデル(generative model)
    - (x,y)を生成するような確率分布を想定
      - 観測データを生成する確率分布を想定する
      # 同時確率を学習する?
    - 学習
      - {(x_i,y_i)}_i を見て、P(X,Y) = P(X|Y)P(Y) (のパラメータ)を学習する
    - 予測
#      - y = argmax_y P(y|x) = argmax_y P(x,y)

    - P(Y)を事前データから求める
    - {(x_i,y_i)}_i を見てP(X|Y)(のパラメータ)を学習する
    - 予測
      - y = argmax_y P(x|y) = argmax_y P(y|x)P(y)

- 構造学習(structured output learning)
  - 教師付き学習の一種
  - 出力が構造を持つ

- 構造
  - ラベル列
  - 木
  - グラフ
  - 順位付構造

* 隠れマルコフモデル(hidden Markov model; HMM)
- 生成モデル

X 文
Y 品詞列

Y = argmax_Y P(Y|X) = argmax_Y P(X,Y)/P(X) = argmax_Y P(X,Y)

品詞→品詞の遷移確率 P_T
  P(Y)   = Π_1^{l+1} P_T(y_i | y_{i-1})
品詞→単語の生成確率 P_E
  P(X|Y) = Π_1^l     P_E(x_i | y_i)

- 配列の性質は局所的な構造の積み重ねによって表現される
  - 連続する2つの出力変数の組 (y_i,y_{i+1})
  - 同じ位置での出力変数と入力変数の組 (y_i,x_i)

P_T,P_E の各値は、タグ付きコーパスなどから学習する

同時確率
  P(X,Y)     = Π     P_E(x_i|y_i)   Π     P_T(y_i|y_{i-1})
対数尤度
  log P(X,Y) = Σ log P_E(x_i|y_i) + Σ log P_T(y_i|y_{i-1})
スコア
  S(X,Y)     = Σ     W E,y_i,x_i  + Σ     W T,y_{i_1},y_i
# 高いものを選びたい

** パラメータ
- 遷移確率および生成確率

** 予測
- HMMによる系列ラベリング問題のViterbiアルゴリズムによる解法
- 動的計画法(dynamic programming; DP)の一種

** モデルの学習
- P(X,Y)を、タグ付きコーパスから学習する

* 条件付確率場(conditional random fields; CRFs)
- 識別モデル
  
- 遷移素性
- 観測素性

- (Θ,Φ(X,Y)) = Σ_f θ_fφ_f(X,Y)
  - 入力xに対して出力yを割り当てることの確信度合い

** 対数線形モデル
- P(Y|X) = exp((Θ,Φ(X,Y))) / Σ_Y exp((Θ,Φ(X,Y)))

** パラメータ
- 遷移素性および観測素性毎の重み

** 予測
- Y = argmax_Y P(Y|X) = argmax_Y log P(Y|X) = argmax_Y log(Θ,Φ(X,Y))
- HMMと同様、動的計画法の適用が可能

** モデルの学習
- 最適化
# 徳居

** 欠点
- パラメータが高次元の場合、最適化に時間がかかる
# 徳居 p.15

* structured Perceptron
- 最適化問題を解くのは諦める
- perceptron で更新

- 学習がargmax操作 (坪井ppt)

- CRFと異なり、確率モデルではない
- mistake bound という指標で評価

** Perceptron
