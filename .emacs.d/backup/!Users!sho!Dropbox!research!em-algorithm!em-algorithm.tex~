\documentclass[]{jsarticle}
\input{$HOME/Dropbox/tempdoc/tex-preamble-jsarticle}

\title{言語処理のための機械学習入門\\3章 クラスタリング}
\author{Sho Yokoi}
%% \title{English IIA E2T01; Professor Hayashi, Brian\\Homework \#1}
%% \author{Yokoi, Sho; 1029-23-2177} % ほか
\date{\today}\西暦

\newcommand*{\V}[1]{\vect{#1}}

\newcommand*{\THETA}{\vect{\theta}}
\newcommand*{\THETAnew}{\THETA^{\mathrm{new}}}
\newcommand*{\THETAold}{\THETA^{\mathrm{old}}}
\newcommand*{\MU}{\vect{\mu}}
\newcommand*{\MUnew}{\MU^{\mathrm{new}}}
\newcommand*{\MUold}{\MU^{\mathrm{old}}}
\newcommand*{\SIGMA}{\vect{\Sigma}}

\begin{document}
\maketitle

\section{クラスタリング}
\term{クラスタリング}{clustering}，とくに\term{ハードクラスタリング}{hard clustering}とは，教師無し学習の一種で，入力データをデータ間の類似度を手がかりにいくつかの\term{クラスタ}{cluster}にグループ分けすることを目的とする．例えば，文書をトピック＝話題別に分類するといったタスクに用いられる．

教師付き学習である\term{分類}{classification}とは異なる．

\bigskip
後半で触れられている混合正規分布の推定は，\term{ソフトクラスタリング}{soft clustering}の一種で，データをうまく説明できる仮説(確率分布)を学習することを目的としている．例えば，「$\mathcal{D}=\{\V{x}_1,\dots,\V{x}_N\}$を見て，$\V{x}_{N+1}$がどの程度の確率でどのような値を取るか言い当てる」「めったに起きない状況(システム異常，不正操作)が分かる」などの利用を見込んだものである．

話の繋がりとして，k-means法→混合正規分布のパラメータ推定を順に考えると分かりやすいが，混合正規分布の推定はデータのグループ分け（ハードクラスタリング）を目的としていないことに注意．

\section{EMアルゴリズムの定式化}
\term{EMアルゴリズム}{expectation-maximization algorithm; EM algorithm}は，観測されない\term{潜在変数}{latent variable}を含む確率モデルにおいて，パラメータの\term{最尤推定量}{maximum likelihood estimator; MLE}
% や\term{最大事後確率}{maximum a posteriori estimator; MAP}
% MAP?
を求める手法である．

すなわち，観測変数$\V{X}$と潜在変数$\V{Z}$の同時分布$p(\V{X},\V{Z};\THETA)$が与えられているとき，尤度関数$L(\THETA)=p(\V{X};\THETA)$を最大化する$\THETA$を求める方法である．

手順は次の通り．
\begin{enumerate}
\item パラメータの初期値$\THETAold$を選ぶ． %選ぶ??
\item Eステップ: $\V{x}\in\mathcal{D}$毎，および$\V{Z}$の実現値$\V{z}$毎に，$p(\V{z}|\V{x};\THETAold)$を計算する．
\item Mステップ: $\THETAnew$を計算する．
\begin{align}
\THETAnew &= \argmax_{\THETA}Q(\THETA;\THETAold)\\
Q(\THETA;\THETAold) &:= \sum_{\V{x}\in\mathcal{D}}\sum_{\V{z}}p(\V{z}|\V{x};\THETAold)\log p(\V{x},\V{z};\THETA)
\end{align}
\item 収束条件を満たしていなければ，$\THETAold := \THETAnew$としてステップ2へ．
\end{enumerate}

% Eステップでは潜在変数の事後確率を求め

\section{混合正規分布の推定}
\subsection{問題設定}
凝集形クラスタリングやk-means法は，一つのデータを一つのクラスタにのみ分類するので\termj{ハードクラスタリング}と呼ばれる．

これに対して，よりソフトな方法として，一つのデータがどのクラスタに属するかが確率的に決まると考えることもできる．
典型的な生成モデルとして，\termj{混合正規分布}を考える．データは，$\pi_k$の確率でクラスタ$k$に落ち，その後$p_k(\V{x})$(平均$\MU_k$分散共分散行列$\SIGMA_k$の多次元($d$次元)正規分布)に従って分布するとする．
\begin{align}
p(\V{x}) &= \sum_k \pi_k p_k(\V{x})\\
p_k(\V{x}) &= \frac{1}{(2\pi)^{d/2}|\SIGMA_k|^{1/2}}\exp\left(-\transpose{(\V{x}-\MU_k)}\SIGMA_k^{-1}(\V{x}-\MU_k)\right)
\end{align}

% これは，与えられたデータ$D=\{\V{x}_1,\dots,\V{x}_N\}$から，パラメータ$\{\pi_k\},\{(\MU_k,\SIGMA_k)\}$を推定する．
\subsection{教科書の問題設定}
教科書では，次のような単純なケースを考えている．
\begin{itemize}
\item $\V{x}$の成分間に相関が無く，かつ次元によらず分散が一定($\SIGMA = \sigma^2 \V{I}_d$)
\item $k$個の正規分布の分散共分散行列が一定($\SIGMA_k = \SIGMA; \sigma_k^2 = \sigma^2$)
\item $k$個の正規分布の混合比が固定($\pi_k$既知)
\end{itemize}

\begin{align}
p(\V{x}) &= \sum_k \pi_k p_k(\V{x})\\
p_k(\V{x}) &= \frac{1}{\sqrt{(2\pi\sigma^2)^d}}\exp\left(- \frac{|\V{x} - \MU_k|^2}{2\sigma^2}\right)
\end{align}

% 与えられたデータ$D=\{\V{x}_1,\dots,\V{x}_N\}$から，パラメータ$\{\MU_k\}$を推定する．
% N?

\section{k-means法の延長として混合正規分布の推定問題を捉える}
\subsection{分布を仮定して，各データの所属クラスを確率的に決める}
\begin{align}
p(c|\V{x}) &= \frac{p(\V{x},c)}{p(\V{x})}\\
&= \frac{p(\V{x},c)}{\sum_c p(\V{x},c)}\\
&= \frac{p(c)p(\V{x}|c)}{\sum_c p(c)p(\V{x}|c)}
\end{align}

\subsection{各データの所属クラスを仮定して，分布を決める}
\begin{align}
\MU_c &= \frac{1}{\text{(クラスcに所属するデータの数)}}\sum_{\text{クラスcに所属する}\V{x}}\V{x}\\
&= \frac{1}{\sum_{\V{x}\in\mathcal{D}}p(c|\V{x})}\sum_{\V{x}\in\mathcal{D}}p(c|\V{x})\V{x}
\end{align}

\section{EMアルゴリズムとして混合正規分布の推定問題を捉える}
\subsection{Eステップ}
\begin{align}
p(c|\V{x};\MUold) &= \frac{p(\V{x},c;\MUold)}{p(\V{x};\MUold)}\\
&= \frac{p(\V{x},c;\MUold)}{\sum_c p(\V{x},c;\MUold)}\\
&= \frac{p(c)p(\V{x}|c;\MUold)}{\sum_c p(c)p(\V{x}|c;\MUold)}
\end{align}
\subsection{Mステップ}
\begin{align}
Q(\MU;\MUold) &= \sum_{\V{x}\in\mathcal{D}}\sum_{c}p(c|\V{x};\MUold)\log p(\V{x},c;\MU)\\
 &= \sum_{\V{x}\in\mathcal{D}}\sum_{c}p(c|\V{x};\MUold)\log p(c)p(\V{x}|c;\MU)
\end{align}
\end{document};
